\documentclass{article}
\input{preamble.tex}

% Authors and affiliations
\title{The Ensemble Kalman Update is an Empirical Matheron Update}

\author{
  Firstname1 Lastname1 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email1@university.edu} \\
  \and
  Firstname2 Lastname2 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email2@university.edu} \\
}

% You may provide any keywords that you find helpful for describing your paper
\date{}

\begin{document}


\begin{abstract}
The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems. In this paper, we show that the ensemble update step of the EnKF is equivalent to an empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we establish this equivalence. This connection provides a probabilistic interpretation of the EnKF and opens avenues for improving ensemble-based data assimilation methods by leveraging properties of the Matheron update.

While this connection is simple, it seems not to be widely known; this note exists to provide a short note expanding on the connection between these facts, together with the necessary definitions so that it is intelligible to a wider audience.
\end{abstract}

\section{Introduction}
The Ensemble Kalman Filter (EnKF)~\citep{Evensen2003Ensemble,Evensen2009Data} is a cornerstone in data assimilation for large-scale dynamical systems due to its computational efficiency and scalability.
The EnKF approximates the state estimation problem by evolving an ensemble of state vectors through the model dynamics and updating them using observational data.

Separately, the Matheron update provides a sample-based method for conditioning Gaussian random variables on observations~\citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.
This approach is well-established in geostatistics and spatial statistics but the connection to ensemble methods in data assimilation seems not to be well-known.

In this paper, we establish that the ensemble update step in the EnKF is equivalent to an empirical Matheron update.
By explicitly representing the ensemble mean and covariance using empirical approximations, we demonstrate this equivalence.
Recognizing this connection provides an alternative  probabilistic foundation for the EnKF and suggests potential improvements in ensemble data assimilation techniques by leveraging the properties of the Matheron update.
Conversely, the analytic Matheron updates in the literature could benefit from the many computational optimisations arising from the data assimilation community.

\section{Background}

\subsection{Notation}

We write random variates sans serif, $\vrv{x}$.

$\Law{\vrv{x}}$ denotes the law of the random variate $\vrv{x}$.
This notation from probability theory is controversial in both machine learning and data assimilation contexts, but it makes life much easier when discussing random variates generated by simulation processes.
Machine-learning types who believe random objects must be discussed only in terms of densities can find a more tedious, less intuitive derivation in sec.~\ref{sec:density-please}.

Mnemonically, samples drawn from the $\Law{\vrv{x}}$ are serif $\vv{x}\sim\Law{\vrv{x}}.$

\subsection{Matheron Update}

The Matheron update is a technique for sampling from the conditional distribution of a Gaussian random variable given observations, without explicitly computing the posterior covariance \citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.

\begin{lemma}[Matheron Update]
Given a jointly Gaussian vector
\begin{align}
    \begin{bmatrix} \vrv{x} \\ \vrv{y} \end{bmatrix}
    &\sim \Normal\left(\begin{bmatrix} \vv{m}_{\vrv{x}} \\ \vv{m}_{\vrv{y}} \end{bmatrix}, \begin{bmatrix} \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\ \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}} \end{bmatrix}\right), \label{eq:joint-gaussian}
\end{align}
the conditional $\vrv{x} | \vrv{y} {=} \vv{y}^*$ is equal in distribution to
\begin{align}
    \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
    &\disteq \vrv{x} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vrv{y} \right).
    \label{eq:matheron-update}
\end{align}
% where $\vv{x}_0$ and $\vv{y}_0$ are samples from the prior distributions of $\vv{x}$ and $\vv{y}$, respectively.
\end{lemma}

\begin{proof}
    A standard property of the Gaussian \citep[e.g.][]{Petersen2012Matrix} is that the conditional distribution of a Gaussian variate  $\vrv{x}$ given $\vrv{y} = \vv{y}^*$ defined as in \eqref{eq:joint-gaussian} is again Gaussian
    \begin{align}
        \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
        \sim\Normal(\vv{m}_{\vrv{x}|\vrv{y}}, \mm{C}_{\vrv{x}|\vrv{y}})
    \end{align}
    with moments
    \begin{align}
        \vv{m}_{\vrv{x}|\vrv{y}}
            &=\Ex [\vrv{x} \gvn \vrv{y} {=} \vv{y}^*] \notag\\
            &= \vv{m}_{\vrv{x}} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vv{m}_{\vrv{y}} \right), \label{eq:conditional-mean}\\
        \mm{C}_{\vrv{x}|\vrv{y}}
            &= \var \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right) \notag\\
            &= \mm{C}_{\vrv{xx}} - \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \mm{C}_{\vrv{yx}}. \label{eq:conditional-cov}
    \end{align}
    % The sample $\vv{x}$ is drawn from $\Normal(\vv{m}_{\vrv{x}|\vrv{y}}, \mm{C}_{\vrv{x}|\vrv{y}})$.
Taking moments of the right hand side of \eqref{eq:matheron-update}
\begin{align}
\Ex\left[\vrv{x}+\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=\vv{m}_{\vrv{x}} +\mm{C}_{\vrv{xy}}\mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vv{m}_{\vrv{y}})\\
&=\vv{m}_{\vrv{x}|\vrv{y}}\\
\var\left[\vrv{x}+\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=
    \var[\vrv{x}]+\var[\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})] \notag \\
    &\hspace{2em} +\var(\vrv{x},\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y}))
    +\var(\vrv{x},\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y}))^{\top}\\
&=\mm{C}_{\vrv{x}\vrv{x}} +\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}
-  2\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}\vrv{x}} -\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}|\vrv{y}}
\end{align}
we see that both first and second moments match.
Since $\Law\left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)$ and
$\Normal(\vv{m}_{\vrv{x}|\vrv{y}}, \mm{C}_{\vrv{x}|\vrv{y}})$ are Gaussian distributions with the same moments, they define the same distribution.
\end{proof}
% Note that this update does not require us to calculate \(\mm{C}_{\vrv{x}\vrv{x}} \)
% and further, may be conducted without needing to evaluate the density of the observation.

This \emph{pathwise} approach to conditioning Gaussian variates has gained currency in machine learning as a tool for sampling and inference in Gaussian proccesses \citep{Wilson2020Efficiently,Wilson2021Pathwise}, notably generalising to challenging domains such as Riemannian manifolds~\citep{Borovitskiy2020Matern}.

where \( \mm{H}:= \left. \nabla_{\vv{x}} \op{H}(\vv{x})\right|_{\vv{x}=\ \) is the observation operator linearized around the ensemble mean, and

\subsection{Ensemble Kalman Filter}

The EnKF is an extension of the classical Kalman Filter designed for nonlinear, high-dimensional systems.
It summarises the estimate of unobserved state $\vrv{x}$ at each time $t$ by an ensemble of $N$ state vectors $\mm{X}_t=[\vv{x}_{t}^{(i)}]_{i=1}^N$.
% At the forward prediction, or \emph{propagation} step, we generate a prior for time $t+1$ as $\mm{X}_{t+1}=[\op{P}(\mathbf{x}_{t}^{(i)})]_{i=1}^N$; we do not address the prediction step in this note.
Since we are not concerned with the time dynamics in this note but only the obversation update step, we suppress the time index hereafter.

The state is typically a spatial field. We represent the values of the field as a one-dimensional finite vector w.l.o.g. by for example representing the field in terms of the weights of some functional basis decomposition.

We assume a known observation operator $\op{H}$ such that observations $\vrv{y}$ are a priori related to state by $\vrv{y}=\op{H}(\vrv{x}\vrv)$; Thus there is a notional joint law
\begin{align}
    \begin{bmatrix}
        \vrv{x}\\
        \vrv{y}
    \end{bmatrix} &\sim\Law \left( \begin{bmatrix}
        \vrv{x}\\
        \op{H}(\vrv{x})
    \end{bmatrix}\right)
    % =:\Law \left( \begin{bmatrix}
    %     \vrv{x}\\
    %     \vrv{y}
    % \end{bmatrix}\right)
\end{align}

In the Ensemble Kalman Filter (EnKF), the **analysis update** is the process of incorporating new observational data into the prior estimate of the system state to obtain an updated, or posterior, estimate. Let \(\vv{y}^*\) represent the observation vector, and let \( \mm{X} \) denote the ensemble of model states whose statistical moments (mean and covariance) form the prior estimate of the state. The goal is to update this ensemble \( \mm{X} \) to a posterior ensemble conditioned on the observation \(\vv{y}^*\).

**Analysis Update Procedure:**

1. **Compute Prior Ensemble Statistics:**
   - **Ensemble Mean:** \( \bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i \)
   - **Ensemble Covariance:** \( \mm{C}_{\vrv{xx}} = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^\top \)

2. **Compute Kalman Gain:**
   The Kalman gain \( K \) determines how much the observations influence the update:
   \[
   K = \mm{C}_{\vrv{xx}} \op{H}^\top (\op{H} \mm{C}_{\vrv{xx}} \op{H}^\top + R)^{-1}
   \]
   where:
   - \( \op{H} \) is the observation operator linearized around the ensemble mean.
   - \( R \) is the observation error covariance matrix.

3. **Update Ensemble Members:**
   For each ensemble member \( x_i \), the updated (posterior) state \( x_i^\text{post} \) is:
   \[
   x_i^\text{post} = x_i + K \left(\vv{y}^*- \op{H} x_i \right)
   \]

**Justification via Conditioning a Gaussian Variate:**

Assuming that both the prior state and observation errors are Gaussian distributed:

- **Prior State Distribution:** \( x \sim \mathcal{N}(\bar{x},  \mm{C}_{\vrv{xx}} ) \)
- **Observation Model:** \(\vv{y}^*= \op{H} x + \varepsilon \), where \( \varepsilon \sim \mathcal{N}(0, R) \)

The **joint distribution** of the state and observation is Gaussian:
\[
\begin{bmatrix}
x \\
y
\end{bmatrix}
\sim \mathcal{N}\left(
\begin{bmatrix}
\bar{x} \\
\op{H} \bar{x}
\end{bmatrix},
\begin{bmatrix}
 \mm{C}_{\vrv{xx}}  & \mm{C}_{\vrv{xx}} \op{H}^\top \\
\op{H} \mm{C}_{\vrv{xx}} & \op{H} \mm{C}_{\vrv{xx}} \op{H}^\top + R
\end{bmatrix}
\right)
\]

The **conditional distribution** of \( x \) given \(\vv{y}^*\) is also Gaussian. The mean and covariance of this conditional distribution are given by:
- **Posterior Mean:**
  \[
  \bar{x}^\text{post} = \bar{x} + K \left(\vv{y}^*- \op{H} \bar{x} \right)
  \]
- **Posterior Covariance:**
  \[
   \mm{C}_{\vrv{xx}} ^\text{post} = \mm{C}_{\vrv{xx}} - K \op{H}  \mm{C}_{\vrv{xx}}
  \]

This means that updating each ensemble member using the Kalman gain \( K \) adjusts the prior estimate towards the observations in a way that is consistent with the Gaussian assumption.

**Updated Posterior Ensemble:**

By applying the above update to each ensemble member, the posterior ensemble \( X^\text{post} \) conditioned on the observation \(\vv{y}^*\) is:
\[
X^\text{post} = \mm{X} + K \left(\vv{y}^*\mathbf{1}^\top - \op{H} \mm{X} \right)
\]
where \( \mathbf{1} \) is a vector of ones corresponding to the ensemble size.

**Conclusion:**

The analysis update in the EnKF effectively "weights" the prior ensemble members according to how well they agree with the new observations, adjusted by the uncertainties in both the model and the observations. This method leverages the properties of Gaussian distributions to ensure that the posterior ensemble accurately reflects the updated state estimate after conditioning on \(\vv{y}^*\).

\section{Equivalence of the EnKF Analysis and the Matheron Update}


In the state filter analysis step, we receive an observation $\vrv{y}=\vv{y}^*$;
Our goal is to characterise the observation-conditional
\begin{align}
    \Law(\vrv{x}^{\text{post}})&:=\Law(\vrv{x}\gvn \vrv{y}{=}\vv{y})
\end{align}
i.e. to find the posterior law given a partial observtion of this joint distribution.

The EnKF introduces two assumptions.
Firstly, that all variates are jointly Gaussian, i.e.
\begin{align}
    \Law \left( \begin{bmatrix}
        \vrv{x}\\
        \vrv{y}
    \end{bmatrix}\right) &\sim \Normal\left(\begin{bmatrix} \vv{m}_{\vrv{x}} \\ \vv{m}_{\vrv{y}} \end{bmatrix}, \begin{bmatrix} \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\ \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}} \end{bmatrix}\right)
\end{align}
Secondly, that these Gaussian distributions may be well approximated by the empirical moments of some ensemble.
We introduce notation for this.

Suppose the $D$-dimensional variate $\vrv{z}$ is distributed as
$\vrv{z}\sim\Normal(\vv{m}_{\vrv{z}},\mm{C}_{\vrv{yy}})$.
Define empirical mean $\mmmean{Z}$ and the prior empirical covariance $\mm{C}_{\vrv{yy}}$ of the ensemble of $N$ stacked column $D$-vectors  $\mm{Z} = \begin{bmatrix}\vv{z}^{(1)}& \vv{z}^{(2)}& \dots& \vv{z}^{(N)}\end{bmatrix}$ as
\begin{align}
    \mmmean{Z} &= \frac{1}{N} \sum_{i=1}^N \vv{z}^{(i)}, \label{eq:ensemble_mean} & \text{ ensemble mean} \\
    \mmdev{Z} &= \frac{1}{\sqrt{N-1}} \left( \mm{Z} - \mmmean{Z} \vv{1}^\top \right) &\text{ deviation}  \label{eq:deviation_matrix} \\
    \hat{\mm{C}}_{\mm{ZZ}} &= \mmdev{Z} \mmdev{Z}^\top + \diag(\vv{\sigma}_{\mm{Z}}^2)&\text{ ensemble covariance}  \label{eq:ensemble-covariance}
\end{align}
where is the state ensemble matrix, $\vv{1}^\top$ is a row vector of $N$ ones and $\vv{\sigma}_{\mm{Z}}^2$ is some $D$-dimensional vector which introduces a regularisation to the empirical covariance.
Overloading notation, we associate a distribution with $\mm{Z}$,
$\Law(\mm{Z}) := \Normal(\mmmean{Z}, \mmdev{Z} \mmdev{Z}^\top + \diag(\vv{\sigma}_{\mm{Z}}^2))$.

If $\Law(\mm{Z})=\Law(\vrv{z})$ then we say they define the same distribution.

In the EnKF, the ensemble representing the prior distribution of observations is
\begin{align}
    \mm{Y} &=: \begin{bmatrix}\op{H}(\vv{x}^{(1)}) & \op{H}(\vv{x}^{(2)})& \dots& \op{H}(\vv{x}^{(N)})\end{bmatrix}\\
    &=: \begin{bmatrix}\vv{y}^{(1)}& \vv{y}^{(2)}& \dots& \vv{y}^{(N)}\end{bmatrix}
\end{align}


The associated empirical joint distribution is
\begin{align}
    \Law\left(\begin{bmatrix}
        \mm{X}\\
        \mm{Y}
    \end{bmatrix}\right) &:= \Normal\left(\begin{bmatrix}
        \mmmean{X}\\
        \mmmean{Y}
    \end{bmatrix},
    \begin{bmatrix}
        \mmdev{X} \mmdev{X}^\top + \diag(\vv{\sigma}_\mm{X}^2) & \mmdev{X} \mmdev{Y}^\top \\
        \mmdev{Y} \mmdev{X}^\top  & \mmdev{Y} \mmdev{Y}^\top + \diag(\vv{\sigma}_\mm{Y}^2)
    \end{bmatrix}
    \right).
\end{align}

For compactness we define the  Kalman gain,
\begin{align}
    \mm{K}
    &= \hat{\mm{C}}_{\mm{XY}} \hat{\mm{C}}_{\mm{YY}}^{-1}\\
    &=\mmdev{X}\mmdev{Y}^\top  \left(
        \mmdev{Y}\mmdev{Y}^\top+\diag(\vv{\sigma}^2_{\mm{Y}})^{-1}
    \right)^{-1}
    \label{eq:kalman_gain}
\end{align}
Suppose we have received the observeration $\vrv{y}=\vv{y}^*$.
Recalling posterior update equations \eqref{eq:conditional-mean} and \eqref{eq:conditional-cov}, and applying these updates in terms of empirical statistics, we recover a posterior
\begin{align}
    \Law\left(\vrv{x}^{\text{post}}\right)
    &=\Law\left(\vrv{x}|\vrv{y}{=}\vv{y}^*\right)\\
    &=\Law\left(\mm{X}|\mm{Y}{=}\vv{y}^*\right)\\
    &= \Normal\left(
        \mmmean{X} +
        \mm{K}\left( \vv{y}^* - \mmmean{Y}\right),
        \hat{\mm{C}}_{\mm{XX}} + \mm{K}\hat{\mm{C}}_{\mm{YX}}
        \right)
\end{align}
Note, however, that it is relatively easy to construct an ensemble with precisely the same covariance by application of the Matheron update~\eqref{eq:matheron-update} via \emph{empirical} moments, to wit,
\begin{align}
    \Law\left(\mm{X}^{\text{post}}\right)
    &=\Law\left(\mm{X}|\mm{Y}{=}\vv{y}^*\right)\\
    &=\Law\left(\mm{X} + \hat{\mm{C}}_{\mm{XY}} \hat{\mm{C}}_{\mm{YY}}^{-1} \left( \vv{y}^*\vv{1} - \mm{Y} \right)\right)\\
    &=\Law\left(\mm{X} -\sqrt{N{-}1\;} \mmdev{X}\mmdev{Y}^\top (\mmdev{Y}\mmdev{Y}^\top+\diag(\vv{\sigma}^2_{\mm{Y}}))^{-1} \mmdev{Y}\right)
\end{align}
Note that $\mm{X}^{\text{post}}$ is once again an $D \times N$ ensemble.

In practice, we would not calculate the distribution niavely, but apply a Woodbury identity TODO

In general, for some distribution where the ensemble size is less than the dimension of the variates, i.e. $N< D$ it is possible that we cannot attain strict equality in distribution.
Then, we may TODO
\citep{Sriperumbudur2010Hilbert}


\section{Computational complexity}

\section{I don't believe you about $\Law$} \label{sec:density-please}

\section{Implications for Data Assimilation}

Understanding the EnKF as an empirical Matheron update opens up several possibilities

\begin{itemize}
    \item \textbf{Improved Covariance Estimation}: Techniques from geostatistics for better covariance estimation can be applied to enhance the EnKF performance.
    \item \textbf{Sampling Methods}: Advanced sampling strategies used in Gaussian process modeling can be integrated into the EnKF framework.
    \item \textbf{Error Characterization}: The probabilistic foundation allows for a better characterization of the errors and uncertainties associated with the EnKF estimates.
    \item \textbf{Ensemble Size Considerations}: Recognizing the EnKF as an empirical method highlights the importance of ensemble size in approximating the true covariances, as discussed in \citet{Fearnhead2018Particle}.
\end{itemize}

\section{Conclusion}

We have demonstrated that the ensemble update step in the EnKF is equivalent to the empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we have established this equivalence. This connection provides a deeper probabilistic understanding of the EnKF and suggests avenues for enhancing ensemble-based data assimilation methods by leveraging the properties of the Matheron update.

\section*{Acknowledgments}

% Acknowledgments can be added here.

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
