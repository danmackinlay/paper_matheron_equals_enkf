\documentclass{article}
\input{preamble.tex}

% Authors and affiliations
\title{The Ensemble Kalman Update is an Empirical Matheron Update}

\author{
  Firstname1 Lastname1 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email1@university.edu} \\
  \and
  Firstname2 Lastname2 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email2@university.edu} \\
}

% You may provide any keywords that you find helpful for describing your paper
\date{}

\begin{document}


\begin{abstract}
The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems. In this paper, we show that the ensemble update step of the EnKF is equivalent to an empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we establish this equivalence. This connection provides a probabilistic interpretation of the EnKF and opens avenues for improving ensemble-based data assimilation methods by leveraging properties of the Matheron update.

While this connection is simple, it seems not to be widely known. Explicit attempts to unify the literatures that gave birth to these concepts are rare, and experts in both domains do not exploit the connection.

This paper exists to provide a short introduciton to the connection, together with the necessary definitions so that it is intelligible to as broad an audience as possible.
\end{abstract}

\section{Introduction}
The Ensemble Kalman Filter (EnKF)~\citep{Evensen2003Ensemble,Evensen2009Data} is a cornerstone in data assimilation for large-scale dynamical systems due to its computational efficiency and scalability.
The EnKF approximates the state estimation problem by evolving an ensemble of state vectors through the model dynamics and updating them using observational data.

Separately, the Matheron update provides a sample-based method for conditioning Gaussian random variables on observations~\citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.
This approach is well-established in geostatistics and spatial statistics but the connection to ensemble methods in data assimilation seems not to be well-known.

In this work, we establish that the ensemble update step in the EnKF is equivalent to an empirical Matheron update, by putting them on a common probabilistic footing.
By explicitly representing the ensemble mean and covariance using empirical approximations, we demonstrate this equivalence.
Recognizing this connection provides an alternative  probabilistic foundation for the EnKF and suggests potential improvements in ensemble data assimilation techniques by leveraging the properties of the Matheron update.
Conversely, the analytic Matheron updates in the literature could benefit from the many computational optimisations arising from the data assimilation community.

\section{Background}

\subsection{Notation}

We write random variates sans serif, $\vrv{x}$.
Equality in distribution is $\disteq$.
The law, or measure, of a random variate $\vrv{x}$ is denoted $\Law[\vrv{x}]$,
Thus $\left(\Law[\vrv{x}]=\Law[\vrv{y}]\right) \Rightarrow \left(\vrv{x}\disteq\vrv{y}\right)$.
Mnemonically, samples drawn from the $\Law[\vrv{x}]$ are written with a serif $\vv{x}\sim\Law$.
We use a hat to denote empirical estimates, e.g. \(\ELaw[\mm{X}]\) is the empirical law induced by the sample matrix \(\mm{X}\).
Where there is no ambiguity we suppress the sample matrix, writing simply \(\widehat{\Law}\).

This notation from probability theory is unpopular in both machine learning and data assimilation contexts, but it makes life easier when discussing random variates generated by simulation processes.
Machine-learning types who prefer  random objects to be discussed only in terms of densities can find an alternative derivation in app.~\ref{sec:density-please}.


\subsection{Matheron Update}

The Matheron update is a technique for sampling from the conditional distribution of a Gaussian random variable given observations, without explicitly computing the posterior covariance \citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.

\begin{lemma}[Matheron Update]
Given a jointly Gaussian vector
\begin{align}
    \begin{bmatrix} \vrv{x} \\ \vrv{y} \end{bmatrix}
    &\sim \Normal\left(\begin{bmatrix} \vv{m}_{\vrv{x}} \\ \vv{m}_{\vrv{y}} \end{bmatrix}, \begin{bmatrix} \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\ \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}} \end{bmatrix}\right), \label{eq:joint-gaussian}
\end{align}
the conditional $\vrv{x} | \vrv{y} {=} \vv{y}^*$ is equal in distribution to
\begin{align}
    \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
    &\disteq \vrv{x} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vrv{y} \right).
    \label{eq:matheron-update}
\end{align}
% where $\vv{x}_0$ and $\vv{y}_0$ are samples from the prior distributions of $\vv{x}$ and $\vv{y}$, respectively.
\end{lemma}

\begin{proof}
    A standard property of the Gaussian \citep[e.g.][]{Petersen2012Matrix} is that the conditional distribution of a Gaussian variate  $\vrv{x}$ given $\vrv{y} = \vv{y}^*$ defined as in \eqref{eq:joint-gaussian} is again Gaussian
    \begin{align}
        \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
        \sim\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})\label{eq:conditional-gaussian}
    \end{align}
    with moments
    \begin{align}
        \vv{m}_{\vrv{x}\gvn\vrv{y}}
            &=\Ex [\vrv{x} \gvn \vrv{y} {=} \vv{y}^*] \notag\\
            &= \vv{m}_{\vrv{x}} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vv{m}_{\vrv{y}} \right), \label{eq:conditional-mean}\\
        \mm{C}_{\vrv{x}\gvn\vrv{y}}
            &= \var \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right) \notag\\
            &= \mm{C}_{\vrv{xx}} - \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \mm{C}_{\vrv{yx}}. \label{eq:conditional-cov}
    \end{align}
    % The sample $\vv{x}$ is drawn from $\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})$.
Taking moments of the right hand side of \eqref{eq:matheron-update}
\begin{align}
\Ex\left[\vrv{x}+\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=\vv{m}_{\vrv{x}} +\mm{C}_{\vrv{xy}}\mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vv{m}_{\vrv{y}})\\
&=\vv{m}_{\vrv{x}\gvn\vrv{y}}\\
\var\left[\vrv{x}+\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=
    \var[\vrv{x}]+\var[\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})] \notag \\
    &\hspace{2em} +\var(\vrv{x},\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y}))
    +\var(\vrv{x},\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y}))^{\top}\\
&=\mm{C}_{\vrv{x}\vrv{x}} +\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}
-  2\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}\vrv{x}} -\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}\gvn\vrv{y}}
\end{align}
we see that both first and second moments match.
Since $\Law\left\{\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right\}$ and
$\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})$ are Gaussian distributions with the same moments, they define the same distribution.
\end{proof}
% Note that this update does not require us to calculate \(\mm{C}_{\vrv{x}\vrv{x}} \)
% and further, may be conducted without needing to evaluate the density of the observation.

This \emph{pathwise} approach to conditioning Gaussian variates has gained currency in machine learning as a tool for sampling and inference in Gaussian proccesses \citep{Wilson2020Efficiently,Wilson2021Pathwise}, notably generalising to challenging domains such as Riemannian manifolds~\citep{Borovitskiy2020Matern}.


\subsection{Kalman Filter}
We begin by recalling that in state filtering the goal is to update our estimate of the system state when a new observation becomes available. In a general filtering problem the objective is to form the posterior distribution \(\Law[\vrv{x}_{t+1} \gvn \vrv{x}_t, \vv{y}^*]\) from the prior \(\Law[\vrv{x}_{t+1} \gvn \vrv{x}_t]\) updating with  the new information in the observation \(\vv{y}^*\).

We assume a known observation operator $\op{H}$ such that observations $\vrv{y}$ are a priori related to state \vrv{x} by $\vrv{y}=\op{H}(\vrv{x}\vrv)$; Thus there exists a joint law for the prior random vector
\begin{align}
    \begin{bmatrix}
        \vrv{x}\\
        \vrv{y}
    \end{bmatrix} &= \begin{bmatrix}
        \vrv{x}\\
        \op{H}(\vrv{x})
    \end{bmatrix}\label{eq:joint-law}
    % \eqcolon\Law \left( \begin{bmatrix}
    %     \vrv{x}\\
    %     \vrv{y}
    % \end{bmatrix}\right)
\end{align}
which is determined by the prior state distribution $\Law[\vrv{x}]$ and the observation operator $\op{H}$.
The  \emph {analysis} step, in state filtering parlance, is the update at time $t$ of  \(\Law[\vrv{x}_{t}]\), into the posterior \( \Law[\vrv{x}_t \gvn (\vrv{y}_t{=}\vv{y}_t^*)]\)
i.e. incorporating the likelihood of the observation $\vv{y}_t^*=\vv{y}_t$.
Although  recursive updating in $t$ is the focus of the classic Kalman filter, in this work we are concerned only with the observational update step.
Hereafter we suppress the time index $t$, and consider an individual analysis update.

Suppose that the state and observation noise are independent, and all variates are defined over a finite dimensional real vector space
$\vv{x}\in\mathbb{R}^{D_{\vrv{x}}}, \vv{y}\in \mathbb{R}^{D_{\vrv{y}}}.$
Suppose, moreover, that at the update step our prior belief about $\vrv{x}$ is Gaussian mean \(\vv{m}_{\vrv{x}}\) and covariance \(\mm{C}_{\vrv{xx}}\), that the observation noise is centred Gaussian with covariance \(\mm{R}\), and the observation operator is linear with matrix \(\mm{H}\), so that the observation is related to the state via
\[
\vv{y} = \mm{H}\,\vv{x} + \vv{\varepsilon}, \quad \vv{\varepsilon} \sim \mathcal{N}(0,\mm{R}).
\]
Then the joint distribution of the prior state and observation is Gaussian and \eqref{eq:joint-law} implies that it is
\begin{align}
\begin{bmatrix}
\vrv{x} \\
\vrv{y}
\end{bmatrix}
&\sim \mathcal{N}\!\left(
    \begin{bmatrix}
    \vv{m}_{\vrv{x}} \\
    \vv{m}_{\vrv{y}}
    \end{bmatrix},
    \begin{bmatrix}
    \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\
    \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}}
    \end{bmatrix}
    \right)\\
&=\mathcal{N}\!\left(
    \begin{bmatrix}
    \vv{m}_{\vrv{x}} \\
    \mm{H}\,\vv{m}_{\vrv{x}}
    \end{bmatrix},
    \begin{bmatrix}
    \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xx}}\,\mm{H}^\top \\
    \mm{H}\,\mm{C}_{\vrv{xx}} & \mm{H}\,\mm{C}_{\vrv{xx}}\,\mm{H}^\top + \mm{R}
    \end{bmatrix}
    \right)
\end{align}
When an observation \(\vv{y}^*\) is obtained, we apply the formulae
\eqref{eq:conditional-gaussian}, \eqref{eq:conditional-mean}, and \eqref{eq:conditional-cov} to calculate
\begin{align}
\left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
&\sim\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})\\
\vv{m}_{\vrv{x}\gvn \vv{y}}
&= \vv{m}_{\vrv{x}} + \mm{K} \left(\vv{y}^* - \vv{m}_{\vrv{y}}\right),\\
&= \vv{m}_{\vrv{x}} + \mm{K} \left(\vv{y}^* - \mm{H}\,\vv{m}_{\vrv{x}}\right),\\
\mm{C}_{\vrv{x}\gvn\vrv{y}}
&= \mm{C}_{\vrv{xx}} - \mm{K}\mm{C}_{\vrv{yx}}\\
&= \mm{C}_{\vrv{xx}} - \mm{K}\,\mm{H}\,\mm{C}_{\vrv{xx}}.
\end{align}
where
\begin{align}
\mm{K}
&\coloneq \mm{C}_{\vrv{xy}}\,\mm{H}^\top \left(\mm{C}_{\vrv{yy}}\right)^{-1},\label{eq:kalman-gain}\\
&\coloneq \mm{C}_{\vrv{xx}}\,\mm{H}^\top \left(\mm{H}\,\mm{C}_{\vrv{xx}}\,\mm{H}^\top + \mm{R}\right)^{-1},
\end{align}
is the \emph{Kalman gain}.

Herafter we consider a diagonal \(\mm{R}\) for simplicity, so that \(\mm{R}=\sigma_{\mm{R}}^2\mm{I}\).

In high-dimensional or nonlinear settings, directly computing these posterior updates is often intractable.
The Ensemble Kalman Filter (EnKF) addresses this issue by representing the belief about the state empirically,  via an ensemble of \(N\) state vectors,
\[
\mm{X} = \begin{bmatrix} \vv{x}^{(1)} & \vv{x}^{(2)} & \cdots & \vv{x}^{(N)} \end{bmatrix},
\]
and by approximating the prior statistics from this ensemble.

For convenience, we introduce the  matrix mean,
\begin{align}
\mmmean{X} &\coloneq \frac{1}{N}\sum_{i=1}^N \vv{x}^{(i)}
\end{align}
and the deviation matrix,
\begin{align}
\mmdev{X} &\coloneq \frac{1}{\sqrt{N-1}} \left( \mm{X} - \mmmean{X} \vv{1}^\top \right), \label{eq:deviation_matrix}
\end{align}
where \(\vv{1}^\top\) is a row vector of \(N\) ones.

The ensemble mean and covariance ae computed from the empirical ensemble samples
\begin{align}
\widehat{\vv{m}}_{\vrv{x}}
&=\mmmean{X}\\
% &\coloneq \frac{1}{N} \sum_{i=1}^{N} \vv{x}^{(i)},\\
\widehat{\mm{C}}_{\vrv{xx}} &\coloneq \frac{1}{N-1} \sum_{i=1}^{N} \left(\vv{x}^{(i)} - \widehat{\vv{m}}_{\vrv{x}}\right)\left(\vv{x}^{(i)} - \widehat{\vv{m}}_{\vrv{x}}\right)^\top\\
&=\mmdev{X} \mmdev{X}^\top + (\mm{I}\sigma^2_{\mm{X}}),\label{eq:ensemble-covariance}
\end{align}

The empirical ensemble estimates of the moments are
\begin{align}
    \widehat{\Ex[\vrv{x}]}=\mmmean{Z} &= \frac{1}{N} \sum_{i=1}^N \vv{z}^{(i)}, \label{eq:ensemble_mean} & \text{ ensemble mean} \\
    \widehat{\var}(\vrv{x}) =\widehat{\mm{C}}_{\mm{ZZ}} &= \mmdev{Z} \mmdev{Z}^\top + (\mm{I}\sigma^2_{\mm{Z}})&\text{ ensemble covariance}  \label{eq:ensemble-covariance}
\end{align}
where is the state ensemble matrix, $\vv{1}^\top$ is a row vector of $N$ ones and $\vv{\sigma}_{\mm{Z}}^2$ is some $D$-dimensional vector which introduces a regularisation to the empirical covariance.
Absuing notation somewhat, we associate a distribution with an ensemble matrix $\mm{Z}$, writing
\begin{align}
\Law[\mm{Z}] \coloneq \Normal(\widehat{\Ex[\vrv{x}]}, \widehat{\var}(\vrv{x})) = \Normal(\mmmean{Z}, \mmdev{Z} \mmdev{Z}^\top + (\mm{I}\sigma^2_{\mm{Z}})).
\end{align}
This has ``concealed'' the stochastic approximation in the hat operator. For the current purposes, as in the classic EnKF literature, we do not concern ourselves deeply with the metric in which we are approximating a target distribution.
In this setting we regard a posterior inference about $\vrv{x}$ given $\vrv{y}=\vv{y}^*$ as successful if we can find  new ensemble $\mm{X}'$ such that, approximately, its empirical moments match the desired target distribution, i.e. we seek to obtain an ensemble $\mm{X}'$ such that
\begin{align}
    \Ex_{\vrv{x}\sim \Law[\mm{X}']} \vrv{x} &\approx \Ex_{\vrv{x}\sim (\Law[\vrv{x} \gvn \vrv{y}=\vv{y}^*])} \vrv{x}\\
    \var_{\vrv{x}\sim \Law[\mm{X}']} \vrv{x} &\approx \var_{\vrv{x}\sim (\Law[\vrv{x} \gvn \vrv{y}=\vv{y}^*])} \vrv{x}
\end{align}
We overload the observation operator to apply to ensemble matrices, writing
\begin{align}
    \op{H}\mm{X} &\coloneq \begin{bmatrix}\op{H}(\vv{x}^{(1)}) & \op{H}(\vv{x}^{(2)})& \dots& \op{H}(\vv{x}^{(N)})\end{bmatrix}.
\end{align}
This also induces an empirical estimate of the observation prior $\Law[\vrv{y}]$,
\begin{align}
    \ELaw[\mm{Y}] &\coloneq \ELaw[{
        \begin{bmatrix}
            \op{H}(\vv{x}^{(1)}) %
            & \op{H}(\vv{x}^{(2)})
            & \dots %
            & \op{H}(\vv{x}^{(N)})
        \end{bmatrix}
        }]\\
    &= \Normal(\mmmean{Y}, \mmdev{Y} \mmdev{Y}^\top + (\mm{I}\sigma^2_{\mm{Y}}))
\end{align}
and in fact an empirical joint
\begin{align}
    \Law\left(\begin{bmatrix}
        \mm{X}\\
        \mm{Y}
    \end{bmatrix}\right) &\coloneq \Normal\left(\begin{bmatrix}
        \mmmean{X}\\
        \mmmean{Y}
    \end{bmatrix},
    \begin{bmatrix}
        \mmdev{X} \mmdev{X}^\top + (\mm{I}\sigma^2_\mm{X}) & \mmdev{X} \mmdev{Y}^\top \\
        \mmdev{Y} \mmdev{X}^\top  & \mmdev{Y} \mmdev{Y}^\top + (\mm{I}\sigma^2_\mm{Y})
    \end{bmatrix}
    \right).\label{eq:ensemble-joint}
\end{align}

When a new observation \(\vv{y}^*\) is available, we form a corresponding observation ensemble.
This is achieved by setting
\[
\mm{Y}^* = \vv{y}^* \mathbf{1}^\top,
\]
so that each column of \(\mm{Y}^*\) equals \(\vv{y}^*\). The forecasted observations are then given by applying the observation operator to the forecast ensemble, namely \(\op{H}\,\mm{X}\).
The Kalman gain in the ensemble setting is constructed by plugging in the empirical ensemble estimates \eqref{eq:ensemble-joint} to \eqref{eq:kalman-gain} obtaining
\begin{align}
\widehat{\mm{K}}= \widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top \left(\op{H}\,\widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top + \mm{R}\right),^{-1}\label{eq:ensemble-kalman-gain}
\end{align}
where \(R\) is the observation error covariance matrix. This gain quantifies the relative weight given to the discrepancy between the actual and forecasted observations.
Finally, the analysis update for the ensemble is performed by
\begin{align}
    \mm{X}' &= \mm{X} + \widehat{\mm{K}} \left(\mm{Y}^* - \op{H}\mm{X}\right).
\end{align}
i.e. each ensemble member is updated by
\begin{align}
\vv{x}^{(i)}{}' \gets \vv{x}^{(i)} + \widehat{\mm{K}} \left(\vv{y}^* - \op{H}\,\vv{x}^{(i)}\right).
\end{align}

Equating moments, we see that
\begin{align}
\ELaw[\mm{X}\gvn \vrv{y}=\vv{y}]= \Law\left\{\mm{X} + \widehat{\mm{K}} \left(\mm{Y}^* - \op{H}\,\mm{X}\right)\right\}.
\end{align}
This equation adjusts each forecast state \(\vv{x}^{(i)}\) by the innovation \(\vv{y}^* - \op{H}\,\vv{x}^{(i)}\), scaled by the Kalman gain \(\mm{K}\), to yield the posterior ensemble \(\mm{X}'\).

\section{Computational complexity}

The computational attractiveness of the method is that the empirical Kalman gain \(\widehat{\mm{K}}\) needed in the ensemble update~\eqref{eq:ensemble-kalman-gain} can be written using only the empirical ensemble statistics, without the need to compute the full covariance matrix \(\mm{C}_{\vrv{xx}}\) by using the empirical moments from~\eqref{eq:ensemble-joint}, so that
\begin{align}
    \widehat{\mm{K}}&= \widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top \left(\op{H}\,\widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top + \mm{R}\right)^{-1}\\
    &= \mmdev{X} \mmdev{Y}^\top   \left(
        \mmdev{Y} \mmdev{Y}^\top + \vv{\sigma}_{\mm{Y}\mm{R}}^2 \mm{I}
    \right)^{-1}\\
    % &= \vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\mm{I}
    % - \vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\mm{I}\mmdev{Y}
    % \left(
    %     \mm{I} + \mmdev{Y}^\top(\vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\mm{I})\mmdev{Y}
    % \right)^{-1}
    % \mmdev{Y}^\top\mm{I}\vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\\
    &= \vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\mm{I}
    - \vv{\sigma}_{\mm{Y}\mm{R}}^{-4}\mmdev{Y}
    \left(
        \mm{I} + \vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\mmdev{Y}^\top\mmdev{Y}
    \right)^{-1}
    \mmdev{Y}^\top
\end{align}
Here we define \( \vv{\sigma}_{\mm{Y}\mm{R}}^2 =\vv{\sigma}_\mm{Y}^2 + \vv{\sigma}_{\mm{R}}^2\) for brevity, and the last line follows from the application of a Woodbury matrix identity.

In the naive Kalman update the computationally dominating cost comes from the solution of the \(D_{\vrv{y}}\times\) system. in the Kalman gain calcuation, where here reduces to   \(\left(
    \mm{I} + \vv{\sigma}_{\mm{Y}\mm{R}}^{-2}\mmdev{Y}^\top\mmdev{Y}
\right)^{-1}
\mmdev{Y}^\top\) requiring the solution of an \(N\times N\) linear system, which if the ensemble size is much smaller than the dimension $N\ll D_{\vrv{y}}$ can be much more efficient, scaling as \(O(D_{\vrv{y}}N^3)\) rather than \(O(D_{\vrv{y}}^3)\).



\citep{Sriperumbudur2010Hilbert}




\section{Implications for Data Assimilation}

Understanding the EnKF as an empirical Matheron update opens up several possibilities

\begin{itemize}
    \item \textbf{Improved Covariance Estimation}: Techniques from geostatistics for better covariance estimation can be applied to enhance the EnKF performance.
    \item \textbf{Sampling Methods}: Advanced sampling strategies used in Gaussian process modeling can be integrated into the EnKF framework.
    \item \textbf{Error Characterization}: The probabilistic foundation allows for a better characterization of the errors and uncertainties associated with the EnKF estimates.
    \item \textbf{Ensemble Size Considerations}: Recognizing the EnKF as an empirical method highlights the importance of ensemble size in approximating the true covariances, as discussed in \citet{Fearnhead2018Particle}.
\end{itemize}

\section{Conclusion}

We have demonstrated that the ensemble update step in the EnKF is equivalent to the empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we have established this equivalence. This connection provides a deeper probabilistic understanding of the EnKF and suggests avenues for enhancing ensemble-based data assimilation methods by leveraging the properties of the Matheron update.

\section*{Acknowledgments}

% Acknowledgments can be added here.

\bibliographystyle{plainnat}
\bibliography{refs}

\appendix

\section{Please use densities rather than $\Law$} \label{sec:density-please}


\end{document}
