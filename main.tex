\documentclass{article}
\input{preamble.tex}

% Authors and affiliations
\title{The Ensemble Kalman Update is an Empirical Matheron Update}

\author{
  Firstname1 Lastname1 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email1@university.edu} \\
  \and
  Firstname2 Lastname2 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email2@university.edu} \\
}

% You may provide any keywords that you find helpful for describing your paper
\date{}

\begin{document}


\begin{abstract}
The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems. In this paper, we show that the ensemble update step of the EnKF is equivalent to an empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we establish this equivalence. This connection provides a probabilistic interpretation of the EnKF and opens avenues for improving ensemble-based data assimilation methods by leveraging properties of the Matheron update.

While this connection is simple, it seems not to be widely known. Explicit attempts to unify the literatures that gave birth to these concepts are rare, and experts in both domains do not exploit the connection.

This paper exists to provide a short introduciton to the connection, together with the necessary definitions so that it is intelligible to as broad an audience as possible.
\end{abstract}

\section{Introduction}
The Ensemble Kalman Filter (EnKF)~\citep{Evensen2003Ensemble,Evensen2009Data} is a cornerstone in data assimilation for large-scale dynamical systems due to its computational efficiency and scalability.
The EnKF approximates the state estimation problem by evolving an ensemble of state vectors through the model dynamics and updating them using observational data.

Separately, the Matheron update provides a sample-based method for conditioning Gaussian random variables on observations~\citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.
This approach is well-established in geostatistics and spatial statistics but the connection to ensemble methods in data assimilation seems not to be well-known.

In this work, we establish that the ensemble update step in the EnKF is equivalent to an empirical Matheron update, by putting them on a common probabilistic footing.
By explicitly representing the ensemble mean and covariance using empirical approximations, we demonstrate this equivalence.
Recognizing this connection provides an alternative  probabilistic foundation for the EnKF and suggests potential improvements in ensemble data assimilation techniques by leveraging the properties of the Matheron update.
Conversely, the analytic Matheron updates in the literature could benefit from the many computational optimisations arising from the data assimilation community.

\subsection{Notation}

We write random variates sans serif, $\vrv{x}$.
Equality in distribution is $\disteq$.
The law, or measure, of a random variate $\vrv{x}$ is denoted $\Law[\vrv{x}]$,
Thus $\left(\Law[\vrv{x}]=\Law[\vrv{y}]\right) \Rightarrow \left(\vrv{x}\disteq\vrv{y}\right)$.
Mnemonically, samples drawn from the $\Law[\vrv{x}]$ are written with a serif $\vv{x}\sim\Law$.
We use a hat to denote empirical estimates, e.g. \(\ELaw[\mm{X}]\) is the empirical law induced by the sample matrix \(\mm{X}\).
Where there is no ambiguity we suppress the sample matrix, writing simply \(\widehat{\Law}\).

This measure notation from probability theory is unpopular in both machine learning and data assimilation contexts who prefer densities to be implied for all random variables.
We attempt a translation table in apprendix~\ref{sec:densities-please}.

\section{Matheron Update}

The Matheron update is a technique for sampling from the conditional distribution of a Gaussian random variable given observations, without explicitly computing the posterior covariance \citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.

\begin{lemma}[Matheron Update]
Given a jointly Gaussian vector
\begin{align}
    \begin{bmatrix} \vrv{x} \\ \vrv{y} \end{bmatrix}
    &\sim \Normal\left(\begin{bmatrix} \vv{m}_{\vrv{x}} \\ \vv{m}_{\vrv{y}} \end{bmatrix}, \begin{bmatrix} \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\ \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}} \end{bmatrix}\right), \label{eq:joint-gaussian}
\end{align}
the conditional $\vrv{x} | \vrv{y} {=} \vv{y}^*$ is equal in distribution to
\begin{align}
    \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
    &\disteq \vrv{x} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vrv{y} \right).
    \label{eq:matheron-update}
\end{align}
% where $\vv{x}_0$ and $\vv{y}_0$ are samples from the prior distributions of $\vv{x}$ and $\vv{y}$, respectively.
\end{lemma}

\begin{proof}
    A standard property of the Gaussian \citep[e.g.][]{Petersen2012Matrix} is that the conditional distribution of a Gaussian variate  $\vrv{x}$ given $\vrv{y} = \vv{y}^*$ defined as in \eqref{eq:joint-gaussian} is again Gaussian
    \begin{align}
        \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
        \sim\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})\label{eq:conditional-gaussian}
    \end{align}
    with moments
    \begin{align}
        \vv{m}_{\vrv{x}\gvn\vrv{y}}
            &=\Ex [\vrv{x} \gvn \vrv{y} {=} \vv{y}^*] \\
            &= \vv{m}_{\vrv{x}} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vv{m}_{\vrv{y}} \right), \label{eq:conditional-mean}\\
        \mm{C}_{\vrv{x}\gvn\vrv{y}}
            &= \var \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right) \\
            &= \mm{C}_{\vrv{xx}} - \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \mm{C}_{\vrv{yx}}. \label{eq:conditional-cov}
    \end{align}
    % The sample $\vv{x}$ is drawn from $\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})$.
Taking moments of the right hand side of \eqref{eq:matheron-update}
\begin{align}
\Ex\left[\vrv{x}+\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=\vv{m}_{\vrv{x}} +\mm{C}_{\vrv{xy}}\mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vv{m}_{\vrv{y}})\\
&=\vv{m}_{\vrv{x}\gvn\vrv{y}}\\
\var\left[\vrv{x}+\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=
    \var[\vrv{x}]+\var[\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y})] \\
    &\hspace{2em} +\var(\vrv{x},\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y}))
    +\var(\vrv{x},\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}(\vv{y}^*-\vrv{y}))^{\top}\\
&=\mm{C}_{\vrv{x}\vrv{x}} +\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}
-  2\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}\vrv{x}} -\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}\gvn\vrv{y}}
\end{align}
we see that both first and second moments match.
Since $\Law\left\{\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right\}$ and
$\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})$ are Gaussian distributions with the same moments, they define the same distribution.
\end{proof}
% Note that this update does not require us to calculate \(\mm{C}_{\vrv{x}\vrv{x}} \)
% and further, may be conducted without needing to evaluate the density of the observation.

This \emph{pathwise} approach to conditioning Gaussian variates has gained currency in machine learning as a tool for sampling and inference in Gaussian proccesses \citep{Wilson2020Efficiently,Wilson2021Pathwise}, notably in generalising to challenging domains such as Riemannian manifolds~\citep{Borovitskiy2020Matern}.


\section{Kalman Filter}
We begin by recalling that in state filtering the goal is to update our estimate of the system state when a new observation becomes available. In a general filtering problem the objective is to form the posterior distribution \(\Law[\vrv{x}_{t+1} \gvn \vrv{x}_t, \vv{y}^*]\) from the prior \(\Law[\vrv{x}_{t+1} \gvn \vrv{x}_t]\) updating with  the new information in the observation \(\vv{y}^*\).

We assume a known observation operator $\op{H}$ such that observations $\vrv{y}$ are a priori related to state \vrv{x} by $\vrv{y}=\op{H}(\vrv{x}\vrv)$; Thus there exists a joint law for the prior random vector
\begin{align}
    \begin{bmatrix}
        \vrv{x}\\
        \vrv{y}
    \end{bmatrix} &= \begin{bmatrix}
        \vrv{x}\\
        \op{H}(\vrv{x})
    \end{bmatrix}\label{eq:joint-law}
    % \eqcolon\Law \left( \begin{bmatrix}
    %     \vrv{x}\\
    %     \vrv{y}
    % \end{bmatrix}\right)
\end{align}
which is determined by the prior state distribution $\Law[\vrv{x}]$ and the observation operator $\op{H}$.
The  \emph {analysis} step, in state filtering parlance, is the update at time $t$ of  \(\Law[\vrv{x}_{t}]\), into the posterior \( \Law[\vrv{x}_t \gvn (\vrv{y}_t{=}\vv{y}_t^*)]\)
i.e. incorporating the likelihood of the observation $\vv{y}_t^*=\vv{y}_t$.
Although  recursive updating in $t$ is the focus of the classic Kalman filter, in this work we are concerned only with the observational update step.
Hereafter we suppress the time index $t$, and consider an individual analysis update.

Suppose that the state and observation noise are independent, and all variates are defined over a finite dimensional real vector space
$\vv{x}\in\mathbb{R}^{D_{\vrv{x}}}, \vv{y}\in \mathbb{R}^{D_{\vrv{y}}}.$
Suppose, moreover, that at the update step our prior belief about $\vrv{x}$ is Gaussian mean \(\vv{m}_{\vrv{x}}\) and covariance \(\mm{C}_{\vrv{xx}}\), that the observation noise is centred Gaussian with covariance \(\mm{R}\), and the observation operator is linear with matrix \(\mm{H}\), so that the observation is related to the state via
\[
\vv{y} = \mm{H}\,\vv{x} + \vv{\varepsilon}, \quad \vv{\varepsilon} \sim \mathcal{N}(0,\mm{R}).
\]
Then the joint distribution of the prior state and observation is Gaussian and \eqref{eq:joint-law} implies that it is
\begin{align}
\begin{bmatrix}
\vrv{x} \\
\vrv{y}
\end{bmatrix}
&\sim \mathcal{N}\!\left(
    \begin{bmatrix}
    \vv{m}_{\vrv{x}} \\
    \vv{m}_{\vrv{y}}
    \end{bmatrix},
    \begin{bmatrix}
    \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\
    \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}}
    \end{bmatrix}
    \right)\\
&=\mathcal{N}\!\left(
    \begin{bmatrix}
    \vv{m}_{\vrv{x}} \\
    \mm{H}\,\vv{m}_{\vrv{x}}
    \end{bmatrix},
    \begin{bmatrix}
    \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xx}}\,\mm{H}^\top \\
    \mm{H}\,\mm{C}_{\vrv{xx}} & \mm{H}\,\mm{C}_{\vrv{xx}}\,\mm{H}^\top + \mm{R}
    \end{bmatrix}
    \right)
\end{align}
When an observation \(\vv{y}^*\) is obtained, we apply the formulae
\eqref{eq:conditional-gaussian}, \eqref{eq:conditional-mean}, and \eqref{eq:conditional-cov} to calculate
\begin{align}
\left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
&\sim\Normal(\vv{m}_{\vrv{x}\gvn\vrv{y}}, \mm{C}_{\vrv{x}\gvn\vrv{y}})\\
\vv{m}_{\vrv{x}\gvn \vv{y}}
&= \vv{m}_{\vrv{x}} + \mm{K} \left(\vv{y}^* - \vv{m}_{\vrv{y}}\right),\\
&= \vv{m}_{\vrv{x}} + \mm{K} \left(\vv{y}^* - \mm{H}\,\vv{m}_{\vrv{x}}\right),\\
\mm{C}_{\vrv{x}\gvn\vrv{y}}
&= \mm{C}_{\vrv{xx}} - \mm{K}\mm{C}_{\vrv{yx}}\\
&= \mm{C}_{\vrv{xx}} - \mm{K}\,\mm{H}\,\mm{C}_{\vrv{xx}}.
\end{align}
where
\begin{align}
\mm{K}
&\coloneq \mm{C}_{\vrv{xy}}\,\mm{H}^\top \left(\mm{C}_{\vrv{yy}}\right)^{-1},\label{eq:kalman-gain}\\
&\coloneq \mm{C}_{\vrv{xx}}\,\mm{H}^\top \left(\mm{H}\,\mm{C}_{\vrv{xx}}\,\mm{H}^\top + \mm{R}\right)^{-1},
\end{align}
is the \emph{Kalman gain}.

Herafter we consider a constant diagonal \(\mm{R}\) for simplicity, so that \(\mm{R}=\rho^2\mm{I}\).

\section{Ensemble Kalman Filter}

In high-dimensional or nonlinear settings, directly computing these posterior updates is often intractable.
The Ensemble Kalman Filter (EnKF) addresses this issue by representing the belief about the state empirically,  via an ensemble of \(N\) state vectors sampled from the prior distribution,
\begin{align}
    \mm{X} = \begin{bmatrix} \vv{x}^{(1)} & \vv{x}^{(2)} & \cdots & \vv{x}^{(N)} \end{bmatrix},
\end{align}
and substituting the empirical measure $\ELaw[\mm{X}]\approx \Law$.

In the Gaussian setting a measure is specified entirely by its first two moments, so we aim to cosntruct empirical measures which match the desired target in terms of these moments.
For convenience, we introduce notation for matrix mean,
\begin{align}
\mmmean{X} &\coloneq \frac{1}{N}\sum_{i=1}^N \vv{x}^{(i)}
\end{align}
and the deviation matrix,
\begin{align}
\mmdev{X} &\coloneq \frac{1}{\sqrt{N-1}} \left( \mm{X} - \mmmean{X} \vv{1}^\top \right), \label{eq:deviation_matrix}
\end{align}
where \(\vv{1}^\top\) is a row vector of \(N\) ones.
The ensemble mean and covariance ae computed from the empirical measure,
\begin{align}
    \widehat{\Ex}[\vrv{x}]\coloneq \Ex_{\vrv{x}\sim \ELaw[\mm{X}]}[ \vrv{x}] =\widehat{\vv{m}}_{\vrv{x}}
&=\mmmean{X}\label{eq:ensemble_mean} & \text{ ensemble mean}\\
% &\coloneq \frac{1}{N} \sum_{i=1}^{N} \vv{x}^{(i)},\\
\widehat{\var}(\vrv{x}) \coloneq\var_{\vrv{x}\sim \ELaw[\mm{X}']} (\vrv{x})=\widehat{\mm{C}}_{\vrv{xx}} &\coloneq \frac{1}{N-1} \sum_{i=1}^{N} \left(\vv{x}^{(i)} - \widehat{\vv{m}}_{\vrv{x}}\right)\left(\vv{x}^{(i)} - \widehat{\vv{m}}_{\vrv{x}}\right)^\top  \\
&=\mmdev{X} \mmdev{X}^\top + (\xi^2\mm{I}),&\text{ ensemble covariance}  \label{eq:ensemble-covariance}
\end{align}
$\vv{\sigma}_{\mm{X}}^2$ is some scalar constant introduces a regularisation to the empirical covariance to ensure it is invertible.
Abusing notation somewhat, we associate a Gaussian distribution with the empirical measure
\begin{align}
\ELaw[\mm{X}] \approx \Normal(\widehat{\Ex}[\vrv{x}], \widehat{\var}(\vrv{x})) = \Normal(\mmmean{X}, \mmdev{X} \mmdev{X}^\top + \xi^2\mm{I}).
\end{align}
This association should be understood purely as a mnemonic device rather than a precise claim about convergence in some metric.
Rigorous convergence of the convergence of such empirical measures to the true distribution is beyond the scope of this short note, but much studied and rather subtle~\citep{LeGland2009Large,Mandel2011Convergence,Kelly2014Wellposedness,Kwiatkowski2015Convergence,DelMoral2017Stability}.
For now, we regard a posterior inference about $\vrv{x}$ given $\vrv{y}=\vv{y}^*$ as successful if we can find  new ensemble $\mm{X}'$ such that, approximately, its empirical moments match the desired target distribution, i.e. we seek to obtain an ensemble $\mm{X}'$ such that
\begin{align}
    \Ex_{\vrv{x}\sim \ELaw[\mm{X}']}[ \vrv{x}] &\approx \Ex_{\vrv{x}\sim (\Law[\vrv{x} \gvn \vrv{y}=\vv{y}^*])} [\vrv{x}]\\
    \var_{\vrv{x}\sim \ELaw[\mm{X}']} (\vrv{x}) &\approx \var_{\vrv{x}\sim (\Law[\vrv{x} \gvn \vrv{y}=\vv{y}^*])} (\vrv{x})
\end{align}
We overload the observation operator to apply to ensemble matrices, writing
\begin{align}
    \mm{Y}\coloneq\op{H}\mm{X} &\coloneq \begin{bmatrix}\op{H}(\vv{x}^{(1)}) & \op{H}(\vv{x}^{(2)})& \dots& \op{H}(\vv{x}^{(N)})\end{bmatrix}.
\end{align}
This also induces an empirical estimate of the observation prior measure $\Law[\vrv{y}]$,
\begin{align}
    \ELaw[\mm{Y}]
        % &\coloneq \ELaw[{
        % \begin{bmatrix}
        %     \op{H}(\vv{x}^{(1)}) %
        %     & \op{H}(\vv{x}^{(2)})
        %     & \dots %
        %     & \op{H}(\vv{x}^{(N)})
        % \end{bmatrix}
        % }]\\
    &\coloneq \Normal(\mmmean{Y}, \mmdev{Y} \mmdev{Y}^\top + (\upsilon^2\mm{I}))
\end{align}
and in fact an empirical joint
\begin{align}
    \ELaw[{\left[\begin{smallmatrix}
        \mm{X}\\
        \mm{Y}
    \end{smallmatrix}\right]}] &\coloneq \Normal\left(\begin{bmatrix}
        \mmmean{X}\\
        \mmmean{Y}
    \end{bmatrix},
    \begin{bmatrix}
        \mmdev{X} \mmdev{X}^\top + \xi^2\mm{I} & \mmdev{X} \mmdev{Y}^\top \\
        \mmdev{Y} \mmdev{X}^\top  & \mmdev{Y} \mmdev{Y}^\top + \upsilon^2\mm{I}
    \end{bmatrix}
    \right).\label{eq:ensemble-joint}
\end{align}

When a new observation \(\vv{y}^*\) is available, we form a corresponding observation ensemble.
This is achieved by setting
\[
\mm{Y}^* = \vv{y}^* \mathbf{1}^\top,
\]
so that each column of \(\mm{Y}^*\) equals \(\vv{y}^*\).
The Kalman gain in the ensemble setting is constructed by plugging in the empirical ensemble estimates \eqref{eq:ensemble-joint} to \eqref{eq:kalman-gain} obtaining
\begin{align}
\widehat{\mm{K}}= \widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top \left(\op{H}\,\widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top + \xi^2\mm{I} + \rho^2\mm{I}\right)^{-1}\label{eq:ensemble-kalman-gain}
\end{align}
where \(R\) is the observation error covariance matrix.
Finally, the analysis update for the ensemble is performed by
\begin{align}
    \mm{X}' &= \mm{X} + \widehat{\mm{K}} \left(\mm{Y}^* - \op{H}\mm{X}\right).\label{eq:ensemble-update}
\end{align}
i.e. each ensemble member is updated
\begin{align}
\vv{x}^{(i)}{}' \gets \vv{x}^{(i)} + \widehat{\mm{K}} \left(\vv{y}^* - \op{H}\,\vv{x}^{(i)}\right).
\end{align}

Equating moments, we see that
\begin{align}
\ELaw[\vrv{x}\sim \mm{X}']
% \Law\left\{\mm{X} + \widehat{\mm{K}} \left(\mm{Y}^* - \op{H}\,\mm{X}\right)\right\}.
\approx \Law[\vrv{x} \gvn \vrv{y}=\vv{y}^*]
\end{align}
That is, the EnKF analysis equations \eqref{eq:ensemble-kalman-gain} and \eqref{eq:ensemble-update} can be justified in terms of an empirical approximation to the Gaussian posterior update.

\section{Empirical Matheron Update is equvalent to Ensemble Kalman Update}
In this setting we now have a trivial equivalence of the Matheron update and the EnKF analysis step.
Under the substitution
\begin{align}
    \vv{m}_{\vrv{x}} &\to \mmmean{X}, & \mm{C}_{\vrv{xx}} &\to \mmdev{X} \mmdev{X}^\top + \xi^2\mm{I},\\
    \vv{m}_{\vrv{y}} &\to \mmmean{Y}, & \mm{C}_{\vrv{yy}} &\to \mmdev{Y} \mmdev{Y}^\top + \upsilon^2\mm{I} + \rho^2\mm{I},\\
    \mm{C}_{\vrv{xy}} &\to \mmdev{X} \mmdev{Y}^\top, & \mm{C}_{\vrv{yx}} &\to \mmdev{Y} \mmdev{X}^\top,
\end{align}
the Matheron update~\eqref{eq:matheron-update} becomes identical to the ensemble update~\eqref{eq:ensemble-update}.

\section{Computational complexity}

The Ensemble Kalman method is favoured both for its robustness and also its effiency.
The computational attractiveness of the method is that the empirical Kalman gain \(\widehat{\mm{K}}\) needed in the ensemble update~\eqref{eq:ensemble-kalman-gain} can be written using only the empirical ensemble statistics, without the need to compute the full covariance matrix \(\mm{C}_{\vrv{xx}}\) by using the empirical moments from~\eqref{eq:ensemble-joint}, so that
\begin{align}
    \widehat{\mm{K}}&= \widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top \left(\op{H}\,\widehat{\mm{C}}_{\vrv{xx}}\, \op{H}^\top + \rho^2\mm{I} + \xi^2\mm{I}\right)^{-1}\\
    &= \mmdev{X} \mmdev{Y}^\top   \left(
        \mmdev{Y} \mmdev{Y}^\top + \gamma^2 \mm{I}
    \right)^{-1}\\
    % &= \gamma^{-2}\mm{I}
    % - \gamma^{-2}\mm{I}\mmdev{Y}
    % \left(
    %     \mm{I} + \mmdev{Y}^\top(\gamma^{-2}\mm{I})\mmdev{Y}
    % \right)^{-1}
    % \mmdev{Y}^\top\mm{I}\gamma^{-2}\\
    &= \gamma^{-2}\mm{I}
    - \gamma^{-4}\mmdev{Y}
    \left(
        \mm{I} + \gamma^{-2}\mmdev{Y}^\top\mmdev{Y}
    \right)^{-1}
    \mmdev{Y}^\top
\end{align}
Here we defined \(\gamma^2 =\upsilon^2 + \rho^2\) for brevity, and the last line follows from the application of a Woodbury matrix identity.

In the naive Kalman update the computationally dominating cost comes from the solution of the \(D_{\vrv{y}}\times\) system. in the Kalman gain calcuation, where here reduces to   \(\left(
    \mm{I} + \gamma^{-2}\mmdev{Y}^\top\mmdev{Y}
\right)^{-1}
\mmdev{Y}^\top\) requiring the solution of an \(N\times N\) linear system, which if the ensemble size is much smaller than the dimension $N\ll D_{\vrv{y}}$ can be much more efficient, scaling as \(O(D_{\vrv{y}}N^3)\) rather than \(O(D_{\vrv{y}}^3)\).

By careful ordering of operations we can avoid entirely the need to construct any \(D_{\vrv{y}}\times D_{\vrv{y}}\) or \(D_{\vrv{x}}\times D_{\vrv{x}}\) matrices
The cost of the ensemble update is \(O(D_{\vrv{x}}D_{\vrv{y}}N^2)\), which is linear in the dimension of the state space and the ensemble size.


\section{Conclusion and Implications}

Understanding the EnKF as an empirical Matheron update opens up several possibilities, as both Matheron updates and Ensemble Kalman methods have been studied extensively in their respective domains.
Both have witnessed many developments in the Bayesian Machine learning setting
\citep{Alzraiee2022Scalable,Chada2022Convergence,Chen2021Autodifferentiable,Chen2023Reducedorder,Dunbar2022Ensemble,Guth2020Ensemble,Huang2022Iterated,Kovachki2019Ensemble,MacKinlay2025Gaussian,Oliver2022Hybrid,Schillings2017Analysis,Schneider2022Ensemble,Spantini2022Coupling}.
If we understand the pathwise update of the Matheron in the setting of the empirical measures used in Ensemble Kalman filters, this suggests the potential to import developments from each of these to the other.

The equivalence between the ensemble Kalman update and the empirical Matheron update offers several exciting avenues for further research. First, it provides a clear probabilistic interpretation of the ensemble update that may inspire improved regularization and localization strategies. For instance, by leveraging recent advances in sample-based conditioning and uncertainty quantification from the machine learning community, one might develop adaptive schemes that mitigate the effects of sampling error in high-dimensional settings.

Furthermore, this connection suggests that ideas developed in the context of Gaussian process inference—such as efficient sampling on Riemannian manifolds or the use of structured covariance approximations—could be imported into ensemble-based data assimilation frameworks. In particular, exploring extensions of the Matheron update beyond the Gaussian case (or for heavy-tailed error distributions) might lead to novel robust EnKF variants.

Finally, the computational benefits of the empirical formulation, notably the reduction to lower-dimensional inversions via the Woodbury identity, can be exploited in large-scale simulations. Future work will involve a systematic study of these benefits in practical applications, as well as a rigorous analysis of the approximation errors introduced by the finite ensemble size.

\section*{Acknowledgments}

% Acknowledgments can be added here.

\bibliographystyle{plainnat}
\bibliography{refs}

\appendix

\section{Measures as densities}\label{sec:densities-please}

In many machine learning and data assimilation applications, probability measures are represented via probability density functions (pdfs) with respect to the Lebesgue measure. For a continuous random variable \(\vrv{x}\) taking values in \(\mathbb{R}^{D_{\vrv{x}}}\), we denote its density by \(p_{\vrv{x}}(\vv{x})\) so that for any measurable set \(A \subset \mathbb{R}^{D_{\vrv{x}}}\),
\begin{equation}
    \Pr\left(\vrv{x} \in A\right)
    = \int_A p_{\vrv{x}}(\vv{x})\,\mathrm{d}\vv{x}.
\end{equation}

The expectation of any measurable function \(f:\mathbb{R}^{D_{\vrv{x}}}\to\mathbb{R}\) is given by
\begin{equation}
    \Ex[f(\vrv{x})] = \int_{\mathbb{R}^{D_{\vrv{x}}}} f(\vv{x})\,p_{\vrv{x}}(\vv{x})\,\mathrm{d}\vv{x}.
\end{equation}

In practice, when using an ensemble of \(N\) samples \(\{\vv{x}^{(i)}\}_{i=1}^N\), the empirical measure can be represented as a sum of Dirac delta functions:
\begin{equation}
    p_{\text{emp}}(\vv{x}) = \frac{1}{N} \sum_{i=1}^N \delta(\vv{x}-\vv{x}^{(i)}).
\end{equation}
Using the sifting property of the Dirac delta, the empirical expectation of \(f\) is
\begin{equation}
    \int_{\mathbb{R}^{D_{\vrv{x}}}} f(\vv{x})\,p_{\text{emp}}(\vv{x})\,\mathrm{d}\vv{x}
    = \frac{1}{N} \sum_{i=1}^N f(\vv{x}^{(i)}).
\end{equation}

Similarly, if the true prior for \(\vrv{x}\) is given by aGaussian density \(p_{\vrv{x}}(\vv{x})\)
\[
    p_{\vrv{x}}(\vv{x}) = \frac{1}{(2\pi)^{D_{\vrv{x}}/2} \,|\mm{C}_{\vrv{xx}}|^{1/2}} \exp\!\left(-\tfrac{1}{2}(\vv{x}-\vv{m}_{\vrv{x}})^\top \mm{C}_{\vrv{xx}}^{-1} (\vv{x}-\vv{m}_{\vrv{x}})\right),
\] then the joint density for \(\begin{bmatrix}\vrv{x}\\\vrv{y}\end{bmatrix}\) is written as
\begin{equation}
    p_{\vrv{x},\vrv{y}}(\vv{x},\vv{y})
    = p_{\vrv{x}}(\vv{x})\,p_{\vrv{y}|\vrv{x}}(\vv{y}|\vv{x}),
\end{equation}
and the conditional density is defined by
\begin{equation}
    p_{\vrv{x}|\vrv{y}}(\vv{x}|\vv{y}^*)
    = \frac{p_{\vrv{x},\vrv{y}}(\vv{x},\vv{y}^*)}{p_{\vrv{y}}(\vv{y}^*)}
    = \frac{p_{\vrv{x}}(\vv{x})\,p_{\vrv{y}|\vrv{x}}(\vv{y}^*|\vv{x})}{\int_{\mathbb{R}^{D_{\vrv{x}}}} p_{\vrv{x}}(\vv{x}')\,p_{\vrv{y}|\vrv{x}}(\vv{y}^*|\vv{x}')\,\mathrm{d}\vv{x}'}.
\end{equation}

Thus, while our earlier sections framed the discussion in terms of probability measures \(\Law[\cdot]\), the corresponding density formulation is fully compatible. For instance, the Matheron update in density form implies that if \(\vrv{x}\) and \(\vrv{y}\) are jointly Gaussian with densities as above, then the conditional density \(p_{\vrv{x}|\vrv{y}}(\vv{x}|\vv{y}^*)\) is given by
\begin{equation}
    p_{\vrv{x}|\vrv{y}}(\vv{x}|\vv{y}^*)
    = \frac{1}{(2\pi)^{D_{\vrv{x}}/2} \,|\mm{C}_{\vrv{x}|\vrv{y}}|^{1/2}} \exp\!\Biggl(-\tfrac{1}{2}\Bigl(\vv{x} - \Bigl[\vv{m}_{\vrv{x}} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} (\vv{y}^*-\vv{m}_{\vrv{y}})\Bigr]\Bigr)^\top \mm{C}_{\vrv{x}|\vrv{y}}^{-1}\Bigl(\vv{x} - \Bigl[\vv{m}_{\vrv{x}} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} (\vv{y}^*-\vv{m}_{\vrv{y}})\Bigr]\Bigr)\Biggr),
\end{equation}
which is equivalent to saying that the random variable
\begin{equation}
    \vv{x} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} (\vv{y}^*-\vv{y})
\end{equation}
has density \(p_{\vrv{x}|\vrv{y}}(\cdot|\vv{y}^*)\), where the randomness is inherited from \(\vv{x}\) and \(\vv{y}\) distributed according to \(p_{\vrv{x},\vrv{y}}(\vv{x},\vv{y})\).



\end{document}
