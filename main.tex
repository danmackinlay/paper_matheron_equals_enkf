\documentclass{article}
\input{preamble.tex}

% Authors and affiliations
\title{The Ensemble Kalman Update is an Empirical Matheron Update}

\author{
  Firstname1 Lastname1 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email1@university.edu} \\
  \and
  Firstname2 Lastname2 \\
  Department of X \\
  University of Y \\
  City, Country \\
  \texttt{email2@university.edu} \\
}

% You may provide any keywords that you find helpful for describing your paper
\date{}

\begin{document}


\begin{abstract}
The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems. In this paper, we show that the ensemble update step of the EnKF is equivalent to an empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we establish this equivalence. This connection provides a probabilistic interpretation of the EnKF and opens avenues for improving ensemble-based data assimilation methods by leveraging properties of the Matheron update.

While this connection is simple, it seems not to be whidely known; this note exists to provide a short note expanding on the connection between these facts.
\end{abstract}

\section{Introduction}
The Ensemble Kalman Filter (EnKF)~\citep{Evensen2003Ensemble,Evensen2009Data} is a cornerstone in data assimilation for large-scale dynamical systems due to its computational efficiency and scalability.
The EnKF approximates the state estimation problem by evolving an ensemble of state vectors through the model dynamics and updating them using observational data.

Separately, the Matheron update provides a sample-based method for conditioning Gaussian random variables on observations~\citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.
This approach is well-established in geostatistics and spatial statistics but the connection to ensemble methods in data assimilation seems not to be well-known.

In this paper, we establish that the ensemble update step in the EnKF is equivalent to an empirical Matheron update.
By explicitly representing the ensemble mean and covariance using empirical approximations, we demonstrate this equivalence.
Recognizing this connection provides an alternative  probabilistic foundation for the EnKF and suggests potential improvements in ensemble data assimilation techniques by leveraging the properties of the Matheron update.
Conversely, the analytic Matheron updates in the literature could benefit from the many computational optimisations arising from the data assimilation community.

\section{Background}

\subsection{Notation}

We write random variates sans serif, $\vrv{x}$.

$\Law{\vrv{x}}$ denotes the law of the random variate $\vrv{x}$.
This notation from probability theory is controversial in both machine learning and data assimilation contexts, but it makes life much easier when discussing random variates generated by simulation processes.
Machine-learning types who believe random objects must be discussed only in terms of densities can find a more tedious, less intuitive derivation in sec.~\ref{sec:density-please}.

Mnemonically, samples drawn from the $\Law{\vrv{x}}$ are serif $\vv{x}\sim\Law{\vrv{x}}.$

\subsection{Matheron Update}

The Matheron update is a technique for sampling from the conditional distribution of a Gaussian random variable given observations, without explicitly computing the posterior covariance \citep{Doucet2010Note,Wilson2020Efficiently,Wilson2021Pathwise}.

\begin{lemma}[Matheron Update]
Given a jointly Gaussian vector
\begin{align}
    \begin{bmatrix} \vrv{x} \\ \vrv{y} \end{bmatrix}
    &\sim \Normal\left(\begin{bmatrix} \vv{m}_{\vrv{x}} \\ \vv{m}_{\vrv{y}} \end{bmatrix}, \begin{bmatrix} \mm{C}_{\vrv{xx}} & \mm{C}_{\vrv{xy}} \\ \mm{C}_{\vrv{yx}} & \mm{C}_{\vrv{yy}} \end{bmatrix}\right), \label{eq:joint-gaussian}
\end{align}
the conditional $\vrv{x} | \vrv{y} {=} \vv{y}^*$ is equal in distribution to
\begin{align}
    \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
    &\disteq \vrv{x} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vrv{y} \right).
    \label{eq:matheron-update}
\end{align}
% where $\vv{x}_0$ and $\vv{y}_0$ are samples from the prior distributions of $\vv{x}$ and $\vv{y}$, respectively.
\end{lemma}

\begin{proof}
    A standard property of the Gaussian \citep[e.g.][]{Petersen2012Matrix} is that the conditional distribution of a Gaussian variate  $\vrv{x}$ given $\vrv{y} = \vv{y}^*$ defined as in \eqref{eq:joint-gaussian} is again Gaussian
    \begin{align}
        \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)
        \sim\Normal(\vv{m}_{\vrv{x}|\vrv{y}}, \mm{C}_{\vrv{x}|\vrv{y}})
    \end{align}
    with moments
    \begin{align}
        \vv{m}_{\vrv{x}|\vrv{y}}
            &=\Ex [\vrv{x} \gvn \vrv{y} {=} \vv{y}^*]\\
            &= \vv{m}_{\vrv{x}} + \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \left( \vv{y}^* - \vv{m}_{\vrv{y}} \right), \\
        \mm{C}_{\vrv{x}|\vrv{y}}
            &= \var \left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)\\
            &= \mm{C}_{\vrv{xx}} - \mm{C}_{\vrv{xy}} \mm{C}_{\vrv{yy}}^{-1} \mm{C}_{\vrv{yx}}.
    \end{align}
    % The sample $\vv{x}$ is drawn from $\Normal(\vv{m}_{\vrv{x}|\vrv{y}}, \mm{C}_{\vrv{x}|\vrv{y}})$.
Taking moments of teh right hand side of \eqref{eq:matheron-update}
\begin{align}
\Ex\left[\vrv{x}+\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=m_{\vrv{x}} +\mm{C}_{\vrv{yx}}\mm{C}_{\vrv{xx}}^{-1}(\vv{y}^*-m_{\vrv{y}})\\
&=\vv{m}_{\vrv{x}|\vrv{y}}\\
\var\left[\vrv{x}+\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}(\vv{y}^*-\vrv{y})\right]
&=
    \var[\vrv{x}]+\var[\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}(\vv{y}^*-\vrv{y})] \notag \\
    &\hspace{2em} +\var(\vrv{x},\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}(\vv{y}^*-\vrv{y}))
    +\var(\vrv{x},\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}(\vv{y}^*-\vrv{y}))^{\top}\\
&=\mm{C}_{\vrv{x}\vrv{x}} +\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}\mm{C}_{\vrv{xx}} \mm{C}_{\vrv{xx}}^{-1}\mm{C}_{\vrv{xy}}
-  2\mm{C}_{\vrv{xy}} \mm{C}_{\vrv{xx}}^{-1}\mm{C}_{\vrv{yx}}\\
&=\mm{C}_{\vrv{x}\vrv{x}} -\mm{C}_{\vrv{yx}} \mm{C}_{\vrv{xx}}^{-1}\mm{C}_{\vrv{xy}}\\
&=\mm{C}_{\vrv{x}|\vrv{y}}
\end{align}
we see that both first and second moments match.
Since $\Law\left(\vrv{x} \gvn \vrv{y} {=} \vv{y}^*\right)$ and
$\Normal(\vv{m}_{\vrv{x}|\vrv{y}}, \mm{C}_{\vrv{x}|\vrv{y}})$ are Gaussian distributions with the same parameters, they define the same distribution.
\end{proof}
% Note that this update does not require us to calculate \(\mm{C}_{\vrv{x}\vrv{x}} \)
% and further, may be conducted without needing to evaluate the density of the observation.

This \emph{pathwise} approach to conditioning Gaussian variates has gained currency in machine learning as a tool for sampling and inference in Gaussian proccesses \citep{Wilson2020Efficiently,Wilson2021Pathwise}, notably generalising to challenging domains such as Riemannian Manifolds~\citep{Borovitskiy2020Matern}.

\subsection{Ensemble Kalman Filter}

The EnKF is an extension of the classical Kalman Filter designed for nonlinear, high-dimensional systems.
It summarises the estimate of unobserved state $\vrv{x}$ at each time $t$ by an ensemble of $N$ state vectors $\mm{X}_t=[\vv{x}_{t}^{(i)}]_{i=1}^N$.
% At the forward prediction, or \emph{propagation} step, we generate a prior for time $t+1$ as $\mm{X}_{t+1}=[\op{P}(\mathbf{x}_{t}^{(i)})]_{i=1}^N$; we do not address the prediction step in this note.
Since we are not concerned with the time dynamics in this note but only the obversation update step, we suppress the time index hereafter.

The state is typically a spatial field. We represent the values of the field as a one-dimensional finite vector w.l.o.g. by for example representing the field in terms of the weights of some functional basis decomposition.

We assume a known observation operator $\op{M}$ such that observations $\vrv{y}$ are a priori related to state by $\vrv{y}=\op{M}(\vrv{x}\vrv)$; Thus there is a notional joint law
\begin{align}
    \begin{bmatrix}
        \vrv{x}\\
        \vrv{y}
    \end{bmatrix} &\sim\Law \left( \begin{bmatrix}
        \vrv{x}\\
        \op{M}(\vrv{x})
    \end{bmatrix}\right)
\end{align}

At each \emph{analysis} step, we receive an observation $\vv{y}\sim\vrv{y}$;
Our goal is then to characterise the observation-conditional
\begin{align}
    \Law(\tilde{\vrv{x}})&:=\Law(\vrv{x}\gvn \vrv{y}{=}\vv{y})
\end{align}
i.e. to find the posterior law given a partial observtion of this joint distribution.

The EnKF introduces two assumptions to make the state filtering problem tractable.
Firstly, we assume that all variates are jointly Gaussian, i.e.
\begin{align}
    \Law \left( \begin{bmatrix}
        \vrv{x}\\
        \op{M}(\vrv{x})
    \end{bmatrix}\right) &\sim \Normal\left(
        \begin{bmatrix}
            \vv{m}_{\vrv{x}}\\
            \vv{m}_{\vrv{y}}
        \end{bmatrix},
        \begin{bmatrix}
            \mm{C}_{\vrv{x}}\\
            \mm{C}_{\vrv{y}}
        \end{bmatrix}
    \right)
\end{align}
Secondly, we represent all these Gaussian distributions in terms of the empirical moments of some ensemble.
Suppose the $D$-dimensional variate $\vrv{z}$ is distributed as
$\vrv{z}\sim\Normal(\vv{m}_{\vrv{z}},\mm{C}_{\vrv{yy}})$.
Define empirical mean $\mmmean{Z}$ and the prior empirical covariance $\mm{C}_{\vrv{yy}}$ of the ensemble  $\mm{Z} = [\vv{z}^{(1)}, \vv{z}^{(2)}, \dots, \vv{z}^{(N)}]$ as
\begin{align}
    \mmmean{Z} &= \frac{1}{N} \sum_{i=1}^N \vv{z}^{(i)}, \label{eq:ensemble_mean} & \text{ ensemble mean} \\
    \mmdev{Z} &= \frac{1}{\sqrt{N-1}} \left( \mm{Z} - \mmmean{Z} \vv{1}^\top \right) &\text{ deviation}  \label{eq:deviation_matrix} \\
    \mm{C}_{\vrv{yy}} &= \mmdev{Z} \mmdev{Z}^\top + \diag(\vv{\sigma}^2)&\text{ ensemble covariance}  \label{eq:ensemble-covariance}
\end{align}
where is the state ensemble matrix, $\vv{1}^\top$ is a row vector of $N$ ones and $\vv{\sigma}^2$ is $D$-dimensional vector.

Overloading notation, we associate a distribution with $\mm{Z}$,
$\Law(\mm{Z}) := \Normal(\mmmean{Z}, \mmdev{Z} \mmdev{Z}^\top + \diag(\vv{\sigma}^2))$.


Similarly, the ensemble representing the prior belief about our observations (before we incorporate those) representing observations is
\begin{equation}
    \mm{Y} = [\vv{y}^{(1)}, \vv{y}^{(2)}, \dots, \vv{y}^{(N)}],
\end{equation}
with ensemble mean and deviations
\begin{align}
    \mmdev{Y} &= \frac{1}{N} \sum_{i=1}^N \vv{y}^{(i)}, \\
    \mmdev{X} &= \frac{1}{\sqrt{N-1}} \left( \mm{Y} - \mmdev{Y} \vv{1}^\top \right).
\end{align}
The empirical covariance is then
\begin{equation}
    \mm{C}_{\vrv{yy}} \approx \mmdev{X} \mmdev{X}^\top.
\end{equation}

The cross-covariance between the state and the observations is approximated by
\begin{equation}
    \mm{C}_{\vrv{xy}} \approx \mmdev{X} \mmdev{X}^\top.
\end{equation}

The Kalman gain $\mm{K}$ is then computed as
\begin{equation}
    \mm{K} = \mm{C}_{\vrv{xy}} \left( \mm{C}_{\vrv{yy}} + \mm{R} \right)^{-1},
    \label{eq:kalman_gain}
\end{equation}
where $\mm{R}$ is the observation error covariance matrix.

Each ensemble member is updated according to
\begin{equation}
    \vv{x}^{(i)} = \vv{x}^{(i)} + \mm{K} \left( \vv{y} + \boldsymbol{\varepsilon}^{(i)} - \vv{y}^{(i)} \right),
    \label{eq:enkf_update}
\end{equation}
where $\vv{y}$ is the observation vector, and $\boldsymbol{\varepsilon}^{(i)}$ is a sample of the observational noise for the $i$-th ensemble member.

\section{Equivalence of the EnKF Update and the Matheron Update}

\subsection{Derivation}

Consider the EnKF update in Equation~\eqref{eq:enkf_update}. Substituting the expression for the Kalman gain from Equation~\eqref{eq:kalman_gain}, we have
\begin{equation}
    \vv{x}^{(i)} = \vv{x}^{(i)} + \mm{C}_{\vrv{xy}} \left( \mm{C}_{\vrv{yy}} + \mm{R} \right)^{-1} \left( \vv{y} + \boldsymbol{\varepsilon}^{(i)} - \vv{y}^{(i)} \right).
    \label{eq:enkf_update_substituted}
\end{equation}

Assuming that $\vv{y}^{(i)} = \mm{H} \vv{x}^{(i)}$, and that $\boldsymbol{\varepsilon}^{(i)}$ is the observational noise, we can combine $\vv{y}^{(i)}$ and $\boldsymbol{\varepsilon}^{(i)}$ into the perturbed observations
\begin{equation}
    \vv{y}^{(i)} = \vv{y} + \boldsymbol{\varepsilon}^{(i)}.
\end{equation}

Then the update becomes
\begin{equation}
    \vv{x}^{(i)} = \vv{x}^{(i)} + \mm{C}_{\vrv{xy}} \left( \mm{C}_{\vrv{yy}} + \mm{R} \right)^{-1} \left( \vv{y}^{(i)} - \vv{y}^{(i)} \right).
    \label{eq:enkf_matheron}
\end{equation}

This is exactly the form of the Matheron update in Equation~\eqref{eq:matheron_update}, where $\vv{x}^{(i)}$ and $\vv{y}^{(i)}$ are samples from the prior distributions of $\vv{x}$ and $\vv{y}$, respectively, and $\vv{y}^{(i)}$ serves as the observed value $\vv{y}^*$. Therefore, the EnKF update is an empirical Matheron update.

\section{Computational complexity}

\section{I don't believe you about $\Law$} \label{sec:density-please}

\section{Implications for Data Assimilation}

Understanding the EnKF as an empirical Matheron update opens up several possibilities

\begin{itemize}
    \item \textbf{Improved Covariance Estimation}: Techniques from geostatistics for better covariance estimation can be applied to enhance the EnKF performance.
    \item \textbf{Sampling Methods}: Advanced sampling strategies used in Gaussian process modeling can be integrated into the EnKF framework.
    \item \textbf{Error Characterization}: The probabilistic foundation allows for a better characterization of the errors and uncertainties associated with the EnKF estimates.
    \item \textbf{Ensemble Size Considerations}: Recognizing the EnKF as an empirical method highlights the importance of ensemble size in approximating the true covariances, as discussed in \citet{Fearnhead2018Particle}.
\end{itemize}

\section{Conclusion}

We have demonstrated that the ensemble update step in the EnKF is equivalent to the empirical Matheron update for Gaussian random variables. By explicitly representing the ensemble mean and covariance using empirical approximations, we have established this equivalence. This connection provides a deeper probabilistic understanding of the EnKF and suggests avenues for enhancing ensemble-based data assimilation methods by leveraging the properties of the Matheron update.

\section*{Acknowledgments}

% Acknowledgments can be added here.

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
