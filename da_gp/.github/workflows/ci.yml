name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  uv-laptop:
    name: "UV Laptop Test"
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
    
    - name: Set up Python
      run: uv python install 3.11
    
    - name: Create environment and install dependencies
      run: |
        uv venv
        uv sync
    
    - name: Run linting
      run: uv run ruff check da_gp/src da_gp/tests
    
    - name: Run type checking
      run: uv run mypy --install-types --non-interactive da_gp/src
    
    - name: Run tests
      run: uv run pytest da_gp/tests/ -v
    
    - name: Test sklearn backend
      run: uv run da-gp --backend sklearn --n_obs 100 --verbose
    
    - name: Generate figures
      run: |
        # Ensure the output directories exist
        mkdir -p data figures

        # Generate timing benchmark (small sweep for CI)
        uv run python da_gp/scripts/bench.py --n_obs_grid 50 100 200 \
             --backends sklearn dapper_enkf --csv data/timing_ci.csv --repeats 2

        # Generate timing plots
        uv run python da_gp/scripts/plot_timing.py data/timing_ci.csv --output-dir figures

        # Posterior plot
        uv run python da_gp/scripts/plot_posterior.py --backends sklearn --n_obs 50

        # Check figures were generated
        ls -la figures/


  performance:
    name: "Performance Benchmark"
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
    
    - name: Set up Python
      run: uv python install 3.11
    
    - name: Install dependencies
      run: |
        uv venv  
        uv pip install -e .[dev]
    
    - name: Run performance tests
      run: |
        source .venv/bin/activate
        echo "backend,n_obs,time,rmse" > benchmark_results.csv
        for n_obs in 100 500 1000 2000; do
          echo "Testing n_obs=$n_obs"
          da-gp --backend sklearn --n_obs $n_obs | grep "^CSV:" | cut -d' ' -f2 >> benchmark_results.csv
        done
    
    - name: Check performance regression
      run: |
        source .venv/bin/activate
        python -c "
        import pandas as pd
        df = pd.read_csv('benchmark_results.csv')
        max_time = df['time'].max()
        print(f'Maximum time: {max_time:.3f}s')
        if max_time > 30.0:  # Fail if any test takes longer than 30s
            raise ValueError(f'Performance regression: {max_time:.3f}s > 30.0s')
        print('Performance tests passed')
        "
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark_results.csv